{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üß≠ Capstone Project: The Learning Compass\n### Google AI Agents Intensive | Option C: Agents for Good\n\n**Project Overview**\nThis project implements a sophisticated **Multi-Agent System** designed to democratize education. It generates high-quality, personalized lesson plans and study materials for any topic and difficulty level.\n\n**The Architecture**\nUnlike a simple chatbot, this system uses a sequential chain of three specialized agents, built with the **Google Agent Development Kit (ADK)**:\n\n1.  **ü§ñ The Planner (Architect):** Uses a retrieval tool to design a structured 3-day curriculum, preventing structural hallucinations.\n2.  **‚úçÔ∏è The Content Creator (Writer):** Takes the blueprint and drafts detailed, engaging educational content. It is equipped with a **PDF Generation Tool** to create tangible artifacts.\n3.  **‚öñÔ∏è The Evaluator (Judge):** Acts as a quality gate. It reviews the content, scores it, and triggers a **Self-Correction Loop** if the quality does not meet the standard.\n\n**Technical Stack**\n* **Framework:** Google ADK (Agent Development Kit)\n* **Model:** Gemini 2.5 Flash Lite\n* **Tools:** Custom Python Tools (Template Retrieval, PDF Generation)\n* **Pattern:** Sequential Orchestration with Feedback Loops","metadata":{}},{"cell_type":"markdown","source":"## Phase 1: Setup & Tool Definition\n\nIn this phase, we initialize the environment and define the custom tools that give our agents \"hands.\"\n\n**Key Tools:**\n* `get_syllabus_template`: A retrieval tool that provides a proven pedagogical structure, ensuring the Planner Agent doesn't invent random formats.\n* `save_lesson_to_pdf`: A capability tool that allows the Content Agent to compile the final lesson into a downloadable PDF file using the `fpdf` library.","metadata":{}},{"cell_type":"code","source":"# --- 1. Install ADK (If not already installed) ---\n!pip install -q google-adk","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install fpdf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T19:46:58.188432Z","iopub.execute_input":"2025-11-30T19:46:58.188794Z","iopub.status.idle":"2025-11-30T19:47:02.642112Z","shell.execute_reply.started":"2025-11-30T19:46:58.188769Z","shell.execute_reply":"2025-11-30T19:47:02.640914Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: fpdf in /usr/local/lib/python3.11/dist-packages (1.7.2)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport json\nimport re\nfrom fpdf import FPDF\nfrom google.genai import types\nfrom google.adk.agents import LlmAgent\nfrom google.adk.models.google_llm import Gemini\nfrom google.adk.runners import InMemoryRunner\nfrom google.adk.tools import google_search # <--- The Star of the Show\n\n# --- 2. Authentication ---\nGOOGLE_API_KEY = None\ntry:\n    from kaggle_secrets import UserSecretsClient\n    GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n    print(\"‚úÖ Authenticated via Kaggle Secrets.\")\nexcept ImportError:\n    try:\n        from dotenv import load_dotenv\n        load_dotenv()\n        GOOGLE_API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\n        print(\"‚úÖ Authenticated via local .env.\")\n    except ImportError: pass\n\nif GOOGLE_API_KEY: os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\nelse: print(\"‚ö†Ô∏è Auth Error: No key found.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T19:47:11.172494Z","iopub.execute_input":"2025-11-30T19:47:11.173003Z","iopub.status.idle":"2025-11-30T19:47:46.222351Z","shell.execute_reply.started":"2025-11-30T19:47:11.172959Z","shell.execute_reply":"2025-11-30T19:47:46.220970Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Authenticated via Kaggle Secrets.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# --- 3. Custom Tools ---\n\ndef save_lesson_to_pdf(filename: str, content: str) -> str:\n    \"\"\"\n    Saves the lesson content to a PDF file.\n    Args:\n        filename: Name of the file (e.g., 'Lesson.pdf').\n        content: The text content to write.\n    \"\"\"\n    try:\n        pdf = FPDF()\n        pdf.add_page()\n        pdf.set_font(\"Arial\", size=12)\n        # Encode to latin-1 to handle special chars safely\n        safe_content = content.encode('latin-1', 'replace').decode('latin-1')\n        pdf.multi_cell(0, 10, txt=safe_content)\n        pdf.output(filename)\n        return f\"Successfully generated PDF: {filename}\"\n    except Exception as e:\n        return f\"Error generating PDF: {e}\"\n\nprint(\"‚úÖ Tools & Environment Ready (Dynamic Search Enabled).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T19:47:50.687025Z","iopub.execute_input":"2025-11-30T19:47:50.687823Z","iopub.status.idle":"2025-11-30T19:47:50.697975Z","shell.execute_reply.started":"2025-11-30T19:47:50.687786Z","shell.execute_reply":"2025-11-30T19:47:50.696470Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Tools & Environment Ready (Dynamic Search Enabled).\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Phase 2: Architecting the Agent Team\n\nHere we define our three specialized agents using the `LlmAgent` class. Each agent is given a specific **Instruction (System Prompt)** and a distinct set of **Tools** to enforce separation of concerns.\n\n* **Planner Agent:** Has exclusive access to the syllabus template.\n* **Content Agent:** Has exclusive access to the PDF generator.\n* **Evaluator Agent:** Is instructed to act as a strict judge and output structured JSON data for programmatic parsing.","metadata":{}},{"cell_type":"code","source":"retry_config = types.HttpRetryOptions(attempts=5, exp_base=7, initial_delay=1, http_status_codes=[429, 500, 503])\n\n# --- AGENT 1: PLANNER (Researcher) ---\nplanner_agent = LlmAgent(\n    name=\"PlannerAgent\",\n    model=Gemini(model=\"gemini-2.5-flash-lite\", retry_options=retry_config),\n    instruction=\"\"\"\n    You are an expert curriculum planner.\n    1. Use the `google_search` tool to research the standard university curriculum for the user's topic.\n    2. Based on your research, create a modern, structured 3-Day Syllabus for the requested difficulty level.\n    3. Output the 3-day plan clearly.\n    \"\"\",\n    tools=[google_search] # <--- Using Google Search for RAG\n)\n\n# --- AGENT 2: CREATOR (Writer) ---\ncontent_agent = LlmAgent(\n    name=\"ContentAgent\",\n    model=Gemini(model=\"gemini-2.5-flash-lite\", retry_options=retry_config),\n    instruction=\"\"\"\n    You are an expert educational content writer.\n    Write a detailed, engaging lesson based on the provided plan.\n    \n    IMPORTANT: When the lesson is approved, you MUST use the `save_lesson_to_pdf` tool \n    to save the final result. Ensure the filename ends in '.pdf'.\n    \"\"\",\n    tools=[save_lesson_to_pdf] \n)\n\n# --- AGENT 3: EVALUATOR (The Judge) ---\nevaluator_agent = LlmAgent(\n    name=\"EvaluatorAgent\",\n    model=Gemini(model=\"gemini-2.5-flash-lite\", retry_options=retry_config),\n    instruction=\"\"\"\n    Strict academic evaluator. Output ONLY valid JSON.\n    Schema: {\"score\": int, \"reasoning\": \"string\", \"status\": \"PASS\"|\"FAIL\"}\n    Pass criteria: Score >= 4.\n    \"\"\"\n)\n\nprint(\"‚úÖ Agents Initialized (Planner is now Web-Connected).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T19:47:53.955086Z","iopub.execute_input":"2025-11-30T19:47:53.955535Z","iopub.status.idle":"2025-11-30T19:47:53.965498Z","shell.execute_reply.started":"2025-11-30T19:47:53.955499Z","shell.execute_reply":"2025-11-30T19:47:53.964251Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Agents Initialized (Planner is now Web-Connected).\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## Phase 3: The Orchestration Logic\n\nThis is the \"brain\" of the application. Instead of a simple linear chain, we implement a **Robust Execution Pipeline**:\n\n1.  **Event Handling:** The ADK returns a stream of events (Function Calls, Responses, Text). We implement a helper function (`get_last_text_response`) to safely extract the model's final text output, ignoring intermediate tool steps.\n2.  **Self-Correction Loop:** If the **Evaluator Agent** gives a \"FAIL\" score, the logic automatically routes the feedback back to the **Content Agent**, prompting a rewrite. This ensures quality without user intervention.\n3.  **Artifact Generation:** Only once the content passes the quality check does the system instruct the agent to generate the PDF.","metadata":{}},{"cell_type":"code","source":"import json\n\n# --- Helper: Extract Text Safely ---\ndef get_last_text_response(events):\n    if not events: return \"Error: No events.\"\n    for event in reversed(events):\n        if event.content and event.content.parts:\n            part = event.content.parts[0]\n            if hasattr(part, 'text') and part.text: return part.text\n    return \"Error: No text found.\"\n\n# --- Helper: Clean JSON ---\ndef parse_json_garbage(text):\n    \"\"\"Extracts JSON from markdown code blocks if present.\"\"\"\n    try:\n        match = re.search(r'\\{.*\\}', text, re.DOTALL)\n        if match: return json.loads(match.group(0))\n        return json.loads(text)\n    except:\n        return {\"score\": 0, \"status\": \"FAIL\", \"reasoning\": \"JSON Parse Error\"}\n\n# --- MAIN ORCHESTRATOR ---\nasync def run_learning_compass(topic, level=\"Beginner\"):\n    print(f\"üß≠ Launching Learning Compass for: {topic} ({level})\\n\")\n    \n    # 1. PLAN (Research Phase)\n    print(\"--- ü§ñ PLANNER: Researching the Web ---\")\n    planner_runner = InMemoryRunner(agent=planner_agent)\n    plan_res = await planner_runner.run_debug(f\"Research and create a syllabus for {topic} ({level})\")\n    syllabus = get_last_text_response(plan_res)\n    print(f\"\\n[Syllabus Generated]:\\n{syllabus[:200]}...\\n\") \n\n    # 2. DRAFT & REFINE LOOP\n    print(\"--- ‚úçÔ∏è CONTENT: Drafting Lesson ---\")\n    content_runner = InMemoryRunner(agent=content_agent)\n    lesson_text = get_last_text_response(await content_runner.run_debug(f\"Write Day 1 for: {syllabus}\"))\n    \n    eval_runner = InMemoryRunner(agent=evaluator_agent)\n    \n    # SELF-CORRECTION LOOP (Max 2 retries)\n    for attempt in range(1, 4):\n        print(f\"\\n--- ‚öñÔ∏è EVALUATOR: Reviewing (Round {attempt}) ---\")\n        eval_res = get_last_text_response(await eval_runner.run_debug(f\"Evaluate: {lesson_text}\"))\n        metrics = parse_json_garbage(eval_res)\n        \n        print(f\"Score: {metrics.get('score')}/5 | Status: {metrics.get('status')}\")\n        \n        if metrics.get('status') == \"PASS\":\n            print(\"‚úÖ Quality Check Passed!\")\n            \n            # 3. SAVE TO FILE\n            print(\"\\n--- üíæ SAVING PDF ARTIFACT ---\")\n            filename = f\"{topic.replace(' ', '_')}_Lesson.pdf\"\n            save_res = await content_runner.run_debug(\n                f\"The lesson is approved. Please save this content to file '{filename}' using your tool.\"\n            )\n            print(f\"System: {get_last_text_response(save_res)}\")\n            break\n            \n        else:\n            if attempt < 3:\n                print(\"‚ö†Ô∏è Quality Check Failed. Requesting Revision...\")\n                feedback = f\"Evaluator feedback: {metrics.get('reasoning')}. Fix these issues.\"\n                lesson_text = get_last_text_response(await content_runner.run_debug(feedback))\n            else:\n                print(\"‚ùå Max retries reached. Creating partial artifact.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T19:47:58.288303Z","iopub.execute_input":"2025-11-30T19:47:58.288682Z","iopub.status.idle":"2025-11-30T19:47:58.301473Z","shell.execute_reply.started":"2025-11-30T19:47:58.288656Z","shell.execute_reply":"2025-11-30T19:47:58.300133Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Phase 4: Execution\n\nLet's run the system! Enter a topic (e.g., \"Transformer Architecture in NLP\") and a difficulty level to see the agents collaborate, critique, and produce a final PDF.","metadata":{}},{"cell_type":"code","source":"# --- EXECUTE ---\ntopic = input(\"Hey! I'm your learning compass. What do you want to learn?: \")\nlevel = input(\"Which level (Beginner, Intermediate, Advanced) would you like?: \")\nawait run_learning_compass(topic, level)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T19:48:03.109688Z","iopub.execute_input":"2025-11-30T19:48:03.110052Z","iopub.status.idle":"2025-11-30T19:48:57.625940Z","shell.execute_reply.started":"2025-11-30T19:48:03.110026Z","shell.execute_reply":"2025-11-30T19:48:57.624668Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"Hey! I'm your learning compass. What do you want to learn?:  Transformer Architecture vs NVIDIA Nemotrons\nWhich level (Beginner, Intermediate, Advanced) would you like?:  Intermediate\n"},{"name":"stdout","text":"üß≠ Launching Learning Compass for: Transformer Architecture vs NVIDIA Nemotrons (Intermediate)\n\n--- ü§ñ PLANNER: Researching the Web ---\n\n ### Created new session: debug_session_id\n\nUser > Research and create a syllabus for Transformer Architecture vs NVIDIA Nemotrons (Intermediate)\nPlannerAgent > ## Transformer Architecture vs. NVIDIA Nemotrons: An Intermediate Deep Dive\n\nThis 3-day syllabus is designed for an intermediate audience with a foundational understanding of deep learning and natural language processing. It aims to provide a comprehensive comparison between the widely adopted Transformer architecture and NVIDIA's innovative Nemotron models, highlighting their core differences, strengths, and emerging applications.\n\n---\n\n### Day 1: Foundations of Transformers and the Rise of Nemotron\n\n**Morning Session (9:00 AM - 12:30 PM)**\n\n*   **9:00 AM - 10:30 AM: Revisiting the Transformer Architecture**\n    *   **Lecture:** A deep dive into the Transformer's encoder-decoder structure, self-attention mechanisms, multi-head attention, positional encoding, and feed-forward networks.\n    *   **Discussion:** Understanding the computational complexity of self-attention, especially with long sequences, and its implications for inference.\n*   **10:30 AM - 10:45 AM: Coffee Break**\n*   **10:45 AM - 12:30 PM: Introducing NVIDIA Nemotron: A Hybrid Approach**\n    *   **Lecture:** Overview of Nemotron-H models, their hybrid architecture combining Transformer and Mamba layers, and the motivation behind this novel design.\n    *   **Case Study:** Analyzing how Nemotron strategically replaces a majority of self-attention layers with Mamba components to achieve efficiency.\n\n**Lunch Break (12:30 PM - 1:30 PM)**\n\n**Afternoon Session (1:30 PM - 5:00 PM)**\n\n*   **1:30 PM - 3:00 PM: Key Architectural Differences and Efficiency Gains**\n    *   **Lecture:** Comparing the computational and memory footprints of Transformer and Nemotron during inference.\n    *   **Demonstration/Code Walkthrough:** Illustrating the structural differences with simplified code snippets (conceptual).\n*   **3:00 PM - 3:15 PM: Coffee Break**\n*   **3:15 PM - 5:00 PM: Performance Benchmarks and Real-World Applications**\n    *   **Lecture:** Examining benchmark results of Nemotron models against Transformer-based models (e.g., Qwen, Llama).\n    *   **Discussion:** Exploring scenarios where Nemotron excels, such as long-context processing.\n\n---\n\n### Day 2: Deep Dive into Nemotron's Innovations and Transformer Variants\n\n**Morning Session (9:00 AM - 12:30 PM)**\n\n*   **9:00 AM - 10:30 AM: Nemotron's Innovations: Mamba and Hybridization**\n    *   **Lecture:** Understanding the Mamba state-space model and its advantages over self-attention for constant computation and memory usage per token.\n    *   **Research Paper Analysis:** Discussing the NVIDIA research behind Nemotron-H's hybrid design and its implications.\n*   **10:30 AM - 10:45 AM: Coffee Break**\n*   **10:45 AM - 12:30 PM: Advanced Nemotron Techniques: Compression and FP8 Training**\n    *   **Lecture:** Exploring techniques like MiniPuzzle for model compression and distillation in Nemotron.\n    *   **Discussion:** The impact of FP8 training on achieving competitive results with reduced precision.\n\n**Lunch Break (12:30 PM - 1:30 PM)**\n\n**Afternoon Session (1:30 PM - 5:00 PM)**\n\n*   **1:30 PM - 3:00 PM: Exploring Transformer Variants and Optimizations**\n    *   **Lecture:** Overview of various Transformer architectures (e.g., BERT, GPT) and techniques to improve their efficiency (e.g., attention approximations, MQA, GQA).\n    *   **Case Study:** Analyzing how Transformers have evolved to handle efficiency challenges.\n*   **3:00 PM - 3:15 PM: Coffee Break**\n*   **3:15 PM - 5:00 PM: Hands-on Session/Code Exploration (Optional)**\n    *   **Activity:** Working with pre-trained Transformer models (e.g., via Hugging Face) or exploring Nemotron model examples.\n    *   **Tooling:** Introduction to frameworks like Hugging Face, NeMo, and Megatron-LM for working with these models.\n\n---\n\n### Day 3: Future Directions, Applications, and Comparative Analysis\n\n**Morning Session (9:00 AM - 12:30 PM)**\n\n*   **9:00 AM - 10:30 AM: Nemotron's Reasoning Capabilities and Agentic AI**\n    *   **Lecture:** Understanding Nemotron's focus on reasoning capabilities and its application in building AI agents.\n    *   **Demonstration:** Showcasing examples of Nemotron in Retrieval-Augmented Generation (RAG) pipelines and agentic workflows.\n*   **10:30 AM - 10:45 AM: Coffee Break**\n*   **10:45 AM - 12:30 PM: Openness and Community Impact**\n    *   **Lecture:** Discussing NVIDIA's strategy of open-sourcing Nemotron models, datasets, and training techniques.\n    *   **Discussion:** The impact of open-source initiatives on AI research and development.\n\n**Lunch Break (12:30 PM - 1:30 PM)**\n\n**Afternoon Session (1:30 PM - 5:00 PM)**\n\n*   **1:30 PM - 3:00 PM: Comparative Strengths and Weaknesses**\n    *   **Panel Discussion/Debate:** A structured debate comparing Transformers and Nemotrons across various criteria:\n        *   Inference speed and cost\n        *   Accuracy and performance on benchmarks\n        *   Long-context handling\n        *   Training efficiency and complexity\n        *   Versatility and breadth of applications\n        *   Multimodality (briefly touching on current trends)\n*   **3:00 PM - 3:15 PM: Coffee Break**\n*   **3:15 PM - 4:30 PM: Future Trends and Emerging Architectures**\n    *   **Lecture:** Exploring the ongoing evolution of LLM architectures, including further hybrid models and efficiency-focused designs.\n    *   **Guest Speaker (Optional):** A brief talk from an industry expert on the future of AI model development.\n*   **4:30 PM - 5:00 PM: Q&A and Wrap-up**\n    *   **Open Forum:** Addressing remaining questions and summarizing key takeaways from the three days.\n\n---\n\n[Syllabus Generated]:\n## Transformer Architecture vs. NVIDIA Nemotrons: An Intermediate Deep Dive\n\nThis 3-day syllabus is designed for an intermediate audience with a foundational understanding of deep learning and natural...\n\n--- ‚úçÔ∏è CONTENT: Drafting Lesson ---\n\n ### Created new session: debug_session_id\n\nUser > Write Day 1 for: ## Transformer Architecture vs. NVIDIA Nemotrons: An Intermediate Deep Dive\n\nThis 3-day syllabus is designed for an intermediate audience with a foundational understanding of deep learning and natural language processing. It aims to provide a comprehensive comparison between the widely adopted Transformer architecture and NVIDIA's innovative Nemotron models, highlighting their core differences, strengths, and emerging applications.\n\n---\n\n### Day 1: Foundations of Transformers and the Rise of Nemotron\n\n**Morning Session (9:00 AM - 12:30 PM)**\n\n*   **9:00 AM - 10:30 AM: Revisiting the Transformer Architecture**\n    *   **Lecture:** A deep dive into the Transformer's encoder-decoder structure, self-attention mechanisms, multi-head attention, positional encoding, and feed-forward networks.\n    *   **Discussion:** Understanding the computational complexity of self-attention, especially with long sequences, and its implications for inference.\n*   **10:30 AM - 10:45 AM: Coffee Break**\n*   **10:45 AM - 12:30 PM: Introducing NVIDIA Nemotron: A Hybrid Approach**\n    *   **Lecture:** Overview of Nemotron-H models, their hybrid architecture combining Transformer and Mamba layers, and the motivation behind this novel design.\n    *   **Case Study:** Analyzing how Nemotron strategically replaces a majority of self-attention layers with Mamba components to achieve efficiency.\n\n**Lunch Break (12:30 PM - 1:30 PM)**\n\n**Afternoon Session (1:30 PM - 5:00 PM)**\n\n*   **1:30 PM - 3:00 PM: Key Architectural Differences and Efficiency Gains**\n    *   **Lecture:** Comparing the computational and memory footprints of Transformer and Nemotron during inference.\n    *   **Demonstration/Code Walkthrough:** Illustrating the structural differences with simplified code snippets (conceptual).\n*   **3:00 PM - 3:15 PM: Coffee Break**\n*   **3:15 PM - 5:00 PM: Performance Benchmarks and Real-World Applications**\n    *   **Lecture:** Examining benchmark results of Nemotron models against Transformer-based models (e.g., Qwen, Llama).\n    *   **Discussion:** Exploring scenarios where Nemotron excels, such as long-context processing.\n\n---\n\n### Day 2: Deep Dive into Nemotron's Innovations and Transformer Variants\n\n**Morning Session (9:00 AM - 12:30 PM)**\n\n*   **9:00 AM - 10:30 AM: Nemotron's Innovations: Mamba and Hybridization**\n    *   **Lecture:** Understanding the Mamba state-space model and its advantages over self-attention for constant computation and memory usage per token.\n    *   **Research Paper Analysis:** Discussing the NVIDIA research behind Nemotron-H's hybrid design and its implications.\n*   **10:30 AM - 10:45 AM: Coffee Break**\n*   **10:45 AM - 12:30 PM: Advanced Nemotron Techniques: Compression and FP8 Training**\n    *   **Lecture:** Exploring techniques like MiniPuzzle for model compression and distillation in Nemotron.\n    *   **Discussion:** The impact of FP8 training on achieving competitive results with reduced precision.\n\n**Lunch Break (12:30 PM - 1:30 PM)**\n\n**Afternoon Session (1:30 PM - 5:00 PM)**\n\n*   **1:30 PM - 3:00 PM: Exploring Transformer Variants and Optimizations**\n    *   **Lecture:** Overview of various Transformer architectures (e.g., BERT, GPT) and techniques to improve their efficiency (e.g., attention approximations, MQA, GQA).\n    *   **Case Study:** Analyzing how Transformers have evolved to handle efficiency challenges.\n*   **3:00 PM - 3:15 PM: Coffee Break**\n*   **3:15 PM - 5:00 PM: Hands-on Session/Code Exploration (Optional)**\n    *   **Activity:** Working with pre-trained Transformer models (e.g., via Hugging Face) or exploring Nemotron model examples.\n    *   **Tooling:** Introduction to frameworks like Hugging Face, NeMo, and Megatron-LM for working with these models.\n\n---\n\n### Day 3: Future Directions, Applications, and Comparative Analysis\n\n**Morning Session (9:00 AM - 12:30 PM)**\n\n*   **9:00 AM - 10:30 AM: Nemotron's Reasoning Capabilities and Agentic AI**\n    *   **Lecture:** Understanding Nemotron's focus on reasoning capabilities and its application in building AI agents.\n    *   **Demonstration:** Showcasing examples of Nemotron in Retrieval-Augmented Generation (RAG) pipelines and agentic workflows.\n*   **10:30 AM - 10:45 AM: Coffee Break**\n*   **10:45 AM - 12:30 PM: Openness and Community Impact**\n    *   **Lecture:** Discussing NVIDIA's strategy of open-sourcing Nemotron models, datasets, and training techniques.\n    *   **Discussion:** The impact of open-source initiatives on AI research and development.\n\n**Lunch Break (12:30 PM - 1:30 PM)**\n\n**Afternoon Session (1:30 PM - 5:00 PM)**\n\n*   **1:30 PM - 3:00 PM: Comparative Strengths and Weaknesses**\n    *   **Panel Discussion/Debate:** A structured debate comparing Transformers and Nemotrons across various criteria:\n        *   Inference speed and cost\n        *   Accuracy and performance on benchmarks\n        *   Long-context handling\n        *   Training efficiency and complexity\n        *   Versatility and breadth of applications\n        *   Multimodality (briefly touching on current trends)\n*   **3:00 PM - 3:15 PM: Coffee Break**\n*   **3:15 PM - 4:30 PM: Future Trends and Emerging Architectures**\n    *   **Lecture:** Exploring the ongoing evolution of LLM architectures, including further hybrid models and efficiency-focused designs.\n    *   **Guest Speaker (Optional):** A brief talk from an industry expert on the future of AI model development.\n*   **4:30 PM - 5:00 PM: Q&A and Wrap-up**\n    *   **Open Forum:** Addressing remaining questions and summarizing key takeaways from the three days.\n\n---\nContentAgent > Okay, here is the Day 1 lesson plan for \"Transformer Architecture vs. NVIDIA Nemotrons: An Intermediate Deep Dive.\"\n\n## Day 1: Foundations of Transformers and the Rise of Nemotron\n\n**Objective:** To establish a strong understanding of the core Transformer architecture and introduce NVIDIA's Nemotron models as a novel, efficient alternative, highlighting their fundamental differences and initial performance advantages.\n\n---\n\n### Morning Session (9:00 AM - 12:30 PM)\n\n**9:00 AM - 10:30 AM: Revisiting the Transformer Architecture**\n\n*   **Lecture:**\n    We begin by revisiting the foundational elements of the Transformer architecture, a paradigm shift in sequence modeling that powers many of today's leading Large Language Models (LLMs). We will dissect its key components:\n    *   **Encoder-Decoder Structure:** Understanding the roles of the encoder in processing input sequences and the decoder in generating output sequences.\n    *   **Self-Attention Mechanism:** This is the heart of the Transformer. We'll delve into how it allows the model to weigh the importance of different words in a sequence when processing any given word, irrespective of their distance.\n    *   **Multi-Head Attention:** Exploring how multiple attention layers work in parallel, enabling the model to focus on different aspects of the relationships between words.\n    *   **Positional Encoding:** Since self-attention is permutation-invariant, we'll discuss how positional encodings are crucial for injecting information about the order of tokens.\n    *   **Feed-Forward Networks:** Understanding their role in processing the attention outputs independently at each position.\n\n*   **Discussion:**\n    The self-attention mechanism, while powerful, comes with a significant computational cost, scaling quadratically with the sequence length ($O(n^2)$). This poses a challenge for processing very long sequences efficiently during inference. We will discuss the implications of this quadratic complexity on latency, memory usage, and the practical limits of deploying Transformer models for tasks requiring extensive context.\n\n**10:30 AM - 10:45 AM: Coffee Break**\n\n**10:45 AM - 12:30 PM: Introducing NVIDIA Nemotron: A Hybrid Approach**\n\n*   **Lecture:**\n    The landscape of LLMs is rapidly evolving, driven by the need for greater efficiency and performance. NVIDIA has introduced the Nemotron models, representing a significant architectural innovation. We will provide an overview of:\n    *   **Nemotron-H Models:** Understanding their position as hybrid models that leverage the strengths of established architectures while introducing novel components.\n    *   **Hybrid Architecture:** The core innovation of Nemotron-H lies in its strategic combination of Transformer layers with Mamba layers. We will explain the rationale behind this hybrid design ‚Äì aiming to retain the powerful expressive capabilities of Transformers while mitigating their computational bottlenecks.\n    *   **Motivation:** The driving force behind Nemotron is the quest for more efficient inference, particularly for long sequences, and achieving competitive performance with reduced computational overhead.\n\n*   **Case Study:**\n    A key aspect of Nemotron's design is its efficient replacement of a substantial portion of the self-attention layers typically found in Transformers with Mamba components. We will analyze how Nemotron strategically integrates these Mamba layers, hypothesizing about the benefits gained in terms of computational efficiency and memory footprint by reducing the reliance on quadratic-complexity self-attention.\n\n---\n\n### Lunch Break (12:30 PM - 1:30 PM)\n\n---\n\n### Afternoon Session (1:30 PM - 5:00 PM)\n\n**1:30 PM - 3:00 PM: Key Architectural Differences and Efficiency Gains**\n\n*   **Lecture:**\n    We will conduct a direct comparison of the architectural blueprints of traditional Transformers and Nemotron models. This session will focus on:\n    *   **Computational Footprint:** Analyzing how the hybrid approach in Nemotron impacts the FLOPs (Floating Point Operations) required during inference compared to a pure Transformer.\n    *   **Memory Footprint:** Examining the differences in memory requirements, particularly the KV cache, which is a significant factor in Transformer inference costs. We'll discuss how Nemotron's architecture aims to alleviate these memory pressures.\n\n*   **Demonstration/Code Walkthrough (Conceptual):**\n    To solidify understanding, we will present simplified, conceptual code snippets. These snippets will not be directly runnable but will illustrate the structural differences. We'll highlight where Mamba layers are integrated within the Nemotron architecture and contrast this with the standard Transformer block, emphasizing the conceptual trade-offs being made.\n\n**3:00 PM - 3:15 PM: Coffee Break**\n\n**3:15 PM - 5:00 PM: Performance Benchmarks and Real-World Applications**\n\n*   **Lecture:**\n    The theoretical advantages of Nemotron need to be validated by empirical results. We will:\n    *   **Examine Benchmark Results:** Present and discuss published benchmark results comparing Nemotron models (e.g., Nemotron-4 8B) against leading Transformer-based models (such as variants of Llama, Qwen, or Mistral). We'll look at metrics like perplexity, accuracy on various NLP tasks, and inference speed.\n    *   **Identify Strengths:** Focus on the specific areas where Nemotron demonstrates significant advantages, such as its ability to handle much longer contexts efficiently without a prohibitive increase in computational cost.\n\n*   **Discussion:**\n    Based on the architectural differences and benchmark results, we will initiate a discussion on the types of real-world applications where Nemotron's efficiency gains and long-context capabilities make it a particularly compelling choice. This could include document summarization, sophisticated question-answering over large knowledge bases, and advanced code generation.\n\n---\n\nThis concludes Day 1. We've laid the groundwork by reviewing the Transformer and introduced Nemotron, setting the stage for a deeper dive into its innovative components and comparative analysis in the following days.\n\n\n--- ‚öñÔ∏è EVALUATOR: Reviewing (Round 1) ---\n\n ### Created new session: debug_session_id\n\nUser > Evaluate: Okay, here is the Day 1 lesson plan for \"Transformer Architecture vs. NVIDIA Nemotrons: An Intermediate Deep Dive.\"\n\n## Day 1: Foundations of Transformers and the Rise of Nemotron\n\n**Objective:** To establish a strong understanding of the core Transformer architecture and introduce NVIDIA's Nemotron models as a novel, efficient alternative, highlighting their fundamental differences and initial performance advantages.\n\n---\n\n### Morning Session (9:00 AM - 12:30 PM)\n\n**9:00 AM - 10:30 AM: Revisiting the Transformer Architecture**\n\n*   **Lecture:**\n    We begin by revisiting the foundational elements of the Transformer architecture, a paradigm shift in sequence modeling that powers many of today's leading Large Language Models (LLMs). We will dissect its key components:\n    *   **Encoder-Decoder Structure:** Understanding the roles of the encoder in processing input sequences and the decoder in generating output sequences.\n    *   **Self-Attention Mechanism:** This is the heart of the Transformer. We'll delve into how it allows the model to weigh the importance of different words in a sequence when processing any given word, irrespective of their distance.\n    *   **Multi-Head Attention:** Exploring how multiple attention layers work in parallel, enabling the model to focus on different aspects of the relationships between words.\n    *   **Positional Encoding:** Since self-attention is permutation-invariant, we'll discuss how positional encodings are crucial for injecting information about the order of tokens.\n    *   **Feed-Forward Networks:** Understanding their role in processing the attention outputs independently at each position.\n\n*   **Discussion:**\n    The self-attention mechanism, while powerful, comes with a significant computational cost, scaling quadratically with the sequence length ($O(n^2)$). This poses a challenge for processing very long sequences efficiently during inference. We will discuss the implications of this quadratic complexity on latency, memory usage, and the practical limits of deploying Transformer models for tasks requiring extensive context.\n\n**10:30 AM - 10:45 AM: Coffee Break**\n\n**10:45 AM - 12:30 PM: Introducing NVIDIA Nemotron: A Hybrid Approach**\n\n*   **Lecture:**\n    The landscape of LLMs is rapidly evolving, driven by the need for greater efficiency and performance. NVIDIA has introduced the Nemotron models, representing a significant architectural innovation. We will provide an overview of:\n    *   **Nemotron-H Models:** Understanding their position as hybrid models that leverage the strengths of established architectures while introducing novel components.\n    *   **Hybrid Architecture:** The core innovation of Nemotron-H lies in its strategic combination of Transformer layers with Mamba layers. We will explain the rationale behind this hybrid design ‚Äì aiming to retain the powerful expressive capabilities of Transformers while mitigating their computational bottlenecks.\n    *   **Motivation:** The driving force behind Nemotron is the quest for more efficient inference, particularly for long sequences, and achieving competitive performance with reduced computational overhead.\n\n*   **Case Study:**\n    A key aspect of Nemotron's design is its efficient replacement of a substantial portion of the self-attention layers typically found in Transformers with Mamba components. We will analyze how Nemotron strategically integrates these Mamba layers, hypothesizing about the benefits gained in terms of computational efficiency and memory footprint by reducing the reliance on quadratic-complexity self-attention.\n\n---\n\n### Lunch Break (12:30 PM - 1:30 PM)\n\n---\n\n### Afternoon Session (1:30 PM - 5:00 PM)\n\n**1:30 PM - 3:00 PM: Key Architectural Differences and Efficiency Gains**\n\n*   **Lecture:**\n    We will conduct a direct comparison of the architectural blueprints of traditional Transformers and Nemotron models. This session will focus on:\n    *   **Computational Footprint:** Analyzing how the hybrid approach in Nemotron impacts the FLOPs (Floating Point Operations) required during inference compared to a pure Transformer.\n    *   **Memory Footprint:** Examining the differences in memory requirements, particularly the KV cache, which is a significant factor in Transformer inference costs. We'll discuss how Nemotron's architecture aims to alleviate these memory pressures.\n\n*   **Demonstration/Code Walkthrough (Conceptual):**\n    To solidify understanding, we will present simplified, conceptual code snippets. These snippets will not be directly runnable but will illustrate the structural differences. We'll highlight where Mamba layers are integrated within the Nemotron architecture and contrast this with the standard Transformer block, emphasizing the conceptual trade-offs being made.\n\n**3:00 PM - 3:15 PM: Coffee Break**\n\n**3:15 PM - 5:00 PM: Performance Benchmarks and Real-World Applications**\n\n*   **Lecture:**\n    The theoretical advantages of Nemotron need to be validated by empirical results. We will:\n    *   **Examine Benchmark Results:** Present and discuss published benchmark results comparing Nemotron models (e.g., Nemotron-4 8B) against leading Transformer-based models (such as variants of Llama, Qwen, or Mistral). We'll look at metrics like perplexity, accuracy on various NLP tasks, and inference speed.\n    *   **Identify Strengths:** Focus on the specific areas where Nemotron demonstrates significant advantages, such as its ability to handle much longer contexts efficiently without a prohibitive increase in computational cost.\n\n*   **Discussion:**\n    Based on the architectural differences and benchmark results, we will initiate a discussion on the types of real-world applications where Nemotron's efficiency gains and long-context capabilities make it a particularly compelling choice. This could include document summarization, sophisticated question-answering over large knowledge bases, and advanced code generation.\n\n---\n\nThis concludes Day 1. We've laid the groundwork by reviewing the Transformer and introduced Nemotron, setting the stage for a deeper dive into its innovative components and comparative analysis in the following days.\n\nEvaluatorAgent > ```json\n{\n \"score\": 5,\n \"reasoning\": \"The lesson plan is comprehensive, well-structured, and clearly addresses the stated objective. It logically progresses from foundational concepts to introducing the new architecture and its implications. The inclusion of specific components, discussions on complexity, conceptual code walkthroughs, and benchmark analysis demonstrates a thorough understanding of the subject matter and effective pedagogical planning. The timing of sessions seems reasonable. The plan covers both theoretical and practical aspects, making it suitable for an intermediate audience.\",\n \"status\": \"PASS\"\n}\n```\nScore: 5/5 | Status: PASS\n‚úÖ Quality Check Passed!\n\n--- üíæ SAVING PDF ARTIFACT ---\n\n ### Continue session: debug_session_id\n\nUser > The lesson is approved. Please save this content to file 'Transformer_Architecture_vs_NVIDIA_Nemotrons_Lesson.pdf' using your tool.\n","output_type":"stream"},{"name":"stderr","text":"WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n","output_type":"stream"},{"name":"stdout","text":"System: Error: No text found.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Project Reflection & Future Work\n\n### Why Agents?\nA single LLM prompt often struggles to maintain structure, accuracy, and formatting simultaneously. By decomposing the task into **Planning**, **Drafting**, and **Evaluating**, we achieve higher quality output. The **Evaluator Agent** acts as a crucial guardrail, catching hallucinations or poor explanations before they reach the user.\n\n### Key Learnings\n* **Schema Generation:** I learned how the ADK automatically converts Python type hints into tool schemas, allowing Gemini to understand *how* to call functions like `save_lesson_to_pdf`.\n* **Resilience:** I implemented robust error handling to manage the complex event streams returned by the ADK, ensuring the system doesn't crash during tool execution.\n\n### Next Steps\nTo take this from prototype to production, I would:\n1.  **Integrate RAG:** Replace the template dictionary with a Vector Database to retrieve curriculum standards from real-world textbooks.\n2.  **Human-in-the-Loop:** Add a UI checkpoint where the user can approve the Syllabus before the Content Agent begins writing.","metadata":{}}]}